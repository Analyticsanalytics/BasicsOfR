#H-Clustering
#In hierarchical clustering, the clusters are formed by each data point starting in its own cluster using the eucledian
#distance. H-Clust is not ideal for large data sets because it will have hard time figuring out the pairwise (euclidean distance )

daily = read.csv("dailykos.csv")
str(daily)

#Find out eucledian distance b/w all the data points. If we dont use any method, it will use complete linkage method
distances = dist(daily, method="euclidean")

#the Ward's method is a minimum variance method, which tries to find compact and spherical clusters.
#it tries to minimize the variance within each cluster and the distance among clusters.
#ward.D method considers centroid distance for clustering the components
# We can also use "average", "complete", "single" or "centroid" distance method
dailyClusters = hclust(distances, method="ward.D")

#plotting the dendogram to choose no. of clusters
plot(dailyClusters)

#clusterGroups is a vector that assigns each intensity value in the dailyClusters vector to a cluster.
#it labels each of the data points of clustermovies in one of the 7 clusters it belongs to by using the cutree function.
clusterGroups = cutree(dailyClusters, k=7)

#Creating different subsets for all the clusters
set1 = subset(daily[1:1545], clusterGroups==1)
set2 = subset(daily[1:1545], clusterGroups==2)
set3 = subset(daily[1:1545], clusterGroups==3)
set4 = subset(daily[1:1545], clusterGroups==4)
set5 = subset(daily[1:1545], clusterGroups==5)
set6 = subset(daily[1:1545], clusterGroups==6)
set7 = subset(daily[1:1545], clusterGroups==7)

#This computes the mean frequency values of each of the elements in cluster 1, and then outputs
#the 6 elements that occur the most frequently. colMeans = means of column
tail(sort(colMeans(set1)))

tail(sort(colMeans(set2)))
tail(sort(colMeans(set3)))
tail(sort(colMeans(set4)))
tail(sort(colMeans(set5)))
tail(sort(colMeans(set6)))
tail(sort(colMeans(set7)))
